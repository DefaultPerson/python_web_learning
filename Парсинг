# ПАРСИНГ

## beautiful soap
* [link 1](https://python-scripts.com/beautifulsoup-html-parsing)  [link 2](https://code.tutsplus.com/ru/tutorials/scraping-webpages-in-python-with-beautiful-soup-the-basics--cms-28211)

## Генерация юзерагента
* from user_agent import generate_user_agent
* headers = {'User-Agent': generate_user_agent(device_type="desktop", os=('mac', 'linux'))}
* page_response = requests.get(page_link, timeout=5, headers=headers)
* Опытные скрейперы могут попробовать установить свой агент на Googlebot User Agent — поисковый робот Google. Большинство веб-сайтов, очевидно, хотят попасть в выдачу Google и пропускают Googlebot.

## Время ожидания запроса
page_response = requests.get(page_link, timeout=5, headers=headers)
* Используйте случайные задержки (например около 2–10 секунд), чтобы избежать блокировки. Особо щепетильным стоит проверить файл robots.txt (как правило, находится на http://<адрес сайта>/robots.txt). Иногда там можно найти параметр Crawl-delay, который говорит, сколько секунд нужно подождать между запросами, чтобы не вредить работе сервера.
## Отлавливание исключний
try:
   page_response = requests.get(page_link, timeout=5)
   if page_response.status_code == 200:
   else:
       print(page_response.status_code)
except requests.Timeout as e:
   print("It is time to timeout")
   print(str(e))
except # other exception

## Смена IP
proxies = {'http' : 'http://10.10.0.0:0000', 
         'https': 'http://120.10.0.0:0000'}
page_response = requests.get(page_link, proxies=proxies, timeout=5) 

## honeypot
* Ловушки для хакеров — это средства для обнаружения сканеров или скреперов. 
Такими средствами могут быть «скрытые» ссылки, которые не видны пользователям, но могут быть извлечены скреперами и/или вэб-спайдерами. Такие ссылки будут иметь набор стилей CSS «display: none», «visibility: hidden» или «color: #fff;», их можно смешивать, задачая цвет фона или даже перемещаясь из видимой области страницы. Как только ваша программа посещает такую ссылку, ваш IP-адрес может быть помечен для дальнейшего расследования или даже мгновенно заблокирован.
* Другой способ обнаружить хакеров — это добавить ссылки с бесконечно глубокими деревьями директорий. В этом случае вам нужно ограничить количество загруженных страниц или ограничить глубину обхода.
## Scrapy
* AutoThrottle - Это расширение для автоматического регулирования скорости обхода на основе нагрузки как сервера Scrapy, так и веб-сайта, на котором выполняется сканирование.
* scrapy-fake-useragent - Использовать случайный User-Agent, предоставляемый fake-useragent для каждого запроса IP-адреса
* scrapy-proxies - Настройка промежуточного ПО прокси-сервера Scrapy для каждого запроса
* https://pythonru.com/biblioteki/sozdanie-parserov-s-pomoshhju-scrapy-i-python
* https://python-scripts.com/scrapy-example
* https://pythonru.com/uroki/scrapy-parsing

## Добавьте referer
* Referer — заголовок HTTP-запроса, который даёт понять, с какого сайта вы пришли. Неплохой вариант — сделать так, чтобы он показывал, будто вы перешли из Google:
* Referer: https://www.google.com/
* Стоит менять referer для веб-сайтов в разных странах: например для России использовать https://www.google.ru/, а не https://www.google.com/. Вместо Google можно подставить адреса соцсетей: Youtube, Facebook, ВКонтакте. Referer поможет сделать так, чтобы запросы выглядели как трафик с того сайта, откуда обычно приходит больше всего посетителей.

## Используйте headless-браузер(обход отпечатков)
Он эмулирует поведение настоящего браузера и поддерживает программное управление. Чаще всего для этих целей выбирают Chrome Headless.

## Подключите программу для решения CAPTCHA
Существуют веб-сайты, которые систематически просят вас подтвердить, что вы не робот, с помощью капч. Обычно капчи отображаются только для подозрительных IP-адресов, и с этим помогут прокси. В остальных же случаях используйте автоматический решатель CAPTCHA — скажем, 2Captcha или AntiCaptcha.

## Аунтентификация по куки
* import 
* requests session = requests.Session() 
* params = {'username': 'username', 'password': 'password'} 
* s=session.post("http://pythonscraping.com/pages/cookies/welcome.php", params) 
* print("Cookie is set to:") 
* print(s.cookies.get_dict()) print("-----------") 
* print("Going to profile page...") 
* s = session.get("http://pythonscraping.com/pages/cookies/profile.php") 
* print(s.text) 

* Простая аунтентификация
* import requests 
* from requests.auth import AuthBase 
* from requests.auth import HTTPBasicAuth 
* auth = HTTPBasicAuth('ryan', 'password') 
* r = requests.post(url="http://pythonscraping.com/pages/auth/login.php", auth= auth) 
* print(r.text)

## Извлечение текста скрытого за Ajax-стеной: 
* from selenium import webdriver 
* import time 
* driver = webdriver.PhantomJS(executable_path='') 
* driver.get("http://pythonscraping.com/pages/javascript/ajaxDemo.html") 
* time.sleep(3) 
* print(driver.find_element_by_id("content").text) 
* driver.close() 

## Простые заголовки
* import requests from bs4 import BeautifulSoup 
* session = requests.Session() 
* headers = {"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome", "Accept":"text/html,application/xhtml+xml,application/xml; q=0.9,image/webp,*/*;q=0.8"} 
* url = "https://www.whatismybrowser.com/developers/what-http-headers-is-my-browser-sending" 
* req = session.get(url, headers=headers) 
* bsObj = BeautifulSoup(req.text) 
* print(bsObj.find("table",{"class":"table-striped"}).get_text)

## Selenium 
* подменга веб драйвера

* граббер тг
* https://github.com/andreyru02/telegram-grabber
* парсинг карт
* https://www.youtube.com/watch?v=DJysDXJLpM8
* чекер
* https://zismo.biz/topic/943273-kak-napisat-cheker-na-python-3-urovnia/
